{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "    MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
      "0  18.0          8         307.0       130.0  3504.0          12.0   \n",
      "1  15.0          8         350.0       165.0  3693.0          11.5   \n",
      "2  18.0          8         318.0       150.0  3436.0          11.0   \n",
      "3  16.0          8         304.0       150.0  3433.0          12.0   \n",
      "4  17.0          8         302.0       140.0  3449.0          10.5   \n",
      "\n",
      "   Model_Year  Origin  \n",
      "0          70       1  \n",
      "1          70       1  \n",
      "2          70       1  \n",
      "3          70       1  \n",
      "4          70       1  \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# get data\n",
    "dataset_path = keras.utils.get_file(\n",
    "    \"auto-mpg.data\",\n",
    "    \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
    ")\n",
    "\n",
    "columns_name = [\n",
    "    \"MPG\", \"Cylinders\", \"Displacement\", \"Horsepower\", \"Weight\",\n",
    "    \"Acceleration\", \"Model_Year\", \"Origin\"\n",
    "]\n",
    "\n",
    "raw_dataset = pd.read_csv(\n",
    "    dataset_path, \n",
    "    names=columns_name,\n",
    "    sep=\" \",\n",
    "    comment=\"\\t\", \n",
    "    skipinitialspace=True,\n",
    "    na_values=\"?\"\n",
    ")\n",
    "\n",
    "print(raw_dataset.head())\n",
    "dataset = raw_dataset.dropna()\n",
    "origin = dataset.pop(\"Origin\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "    MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n0  18.0          8         307.0       130.0  3504.0          12.0   \n1  15.0          8         350.0       165.0  3693.0          11.5   \n2  18.0          8         318.0       150.0  3436.0          11.0   \n3  16.0          8         304.0       150.0  3433.0          12.0   \n4  17.0          8         302.0       140.0  3449.0          10.5   \n\n   Model_Year  USA  Europe  Japan  \n0          70  1.0     0.0    0.0  \n1          70  1.0     0.0    0.0  \n2          70  1.0     0.0    0.0  \n3          70  1.0     0.0    0.0  \n4          70  1.0     0.0    0.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MPG</th>\n      <th>Cylinders</th>\n      <th>Displacement</th>\n      <th>Horsepower</th>\n      <th>Weight</th>\n      <th>Acceleration</th>\n      <th>Model_Year</th>\n      <th>USA</th>\n      <th>Europe</th>\n      <th>Japan</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18.0</td>\n      <td>8</td>\n      <td>307.0</td>\n      <td>130.0</td>\n      <td>3504.0</td>\n      <td>12.0</td>\n      <td>70</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15.0</td>\n      <td>8</td>\n      <td>350.0</td>\n      <td>165.0</td>\n      <td>3693.0</td>\n      <td>11.5</td>\n      <td>70</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18.0</td>\n      <td>8</td>\n      <td>318.0</td>\n      <td>150.0</td>\n      <td>3436.0</td>\n      <td>11.0</td>\n      <td>70</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16.0</td>\n      <td>8</td>\n      <td>304.0</td>\n      <td>150.0</td>\n      <td>3433.0</td>\n      <td>12.0</td>\n      <td>70</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17.0</td>\n      <td>8</td>\n      <td>302.0</td>\n      <td>140.0</td>\n      <td>3449.0</td>\n      <td>10.5</td>\n      <td>70</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 61
    }
   ],
   "source": [
    "encode_origin = tf.one_hot(origin, depth=4).numpy()\n",
    "sub_dataset = pd.DataFrame(encode_origin[:, 1:], columns=[\"USA\", \"Europe\", \"Japan\"])\n",
    "dataset.index = sub_dataset.index\n",
    "dataset=pd.concat([dataset, sub_dataset], axis=1)\n",
    "\n",
    "dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "x_train: (314, 9), x_test: (78, 9)\n",
      "y_train: (314,), y_test: (78,)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "x_train = dataset.sample(frac=0.8, random_state=0)\n",
    "x_test = dataset.drop(x_train.index)\n",
    "\n",
    "y_train = x_train.pop(\"MPG\")\n",
    "y_test = x_test.pop(\"MPG\")\n",
    "\n",
    "print(f\"x_train: {x_train.shape}, x_test: {x_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}, y_test: {y_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "train_stats = x_train.describe().T\n",
    "\n",
    "norm = lambda x: (x - train_stats[\"mean\"]) / train_stats[\"std\"]\n",
    "\n",
    "normed_x_train = norm(x_train)\n",
    "normed_x_test = norm(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "<BatchDataset shapes: ((None, 9), (None,)), types: (tf.float64, tf.float64)>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 64
    }
   ],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices(\n",
    "    (normed_x_train.values, y_train.values)\n",
    ")\n",
    "train_db = train_db.shuffle(100).batch(32)\n",
    "train_db"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "class Network(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = layers.Dense(64, activation=\"relu\")\n",
    "        self.layer_2 = layers.Dense(64, activation=\"relu\")\n",
    "        self.output_layer = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        layer1 = self.layer_1(inputs)\n",
    "        layer2 = self.layer_2(layer1)\n",
    "        output = self.output_layer(layer2)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model: \"network_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             multiple                  640       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 4,865\n",
      "Trainable params: 4,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model = Network()\n",
    "model.build(input_shape=(None, 9))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "epoch: 0 -- step: 0 -- mse_loss: 75.29277038574219\n",
      "======================================================================\n",
      "epoch: 0 -- step: 5 -- mse_loss: 53.45249938964844\n",
      "======================================================================\n",
      "epoch: 1 -- step: 0 -- mse_loss: 54.07611083984375\n",
      "======================================================================\n",
      "epoch: 1 -- step: 5 -- mse_loss: 55.3983268737793\n",
      "======================================================================\n",
      "epoch: 2 -- step: 0 -- mse_loss: 67.46318054199219\n",
      "======================================================================\n",
      "epoch: 2 -- step: 5 -- mse_loss: 74.72702026367188\n",
      "======================================================================\n",
      "epoch: 3 -- step: 0 -- mse_loss: 59.32172393798828\n",
      "======================================================================\n",
      "epoch: 3 -- step: 5 -- mse_loss: 66.74610900878906\n",
      "======================================================================\n",
      "epoch: 4 -- step: 0 -- mse_loss: 57.281585693359375\n",
      "======================================================================\n",
      "epoch: 4 -- step: 5 -- mse_loss: 57.17613983154297\n",
      "======================================================================\n",
      "epoch: 5 -- step: 0 -- mse_loss: 64.70547485351562\n",
      "======================================================================\n",
      "epoch: 5 -- step: 5 -- mse_loss: 59.26581573486328\n",
      "======================================================================\n",
      "epoch: 6 -- step: 0 -- mse_loss: 51.80408477783203\n",
      "======================================================================\n",
      "epoch: 6 -- step: 5 -- mse_loss: 59.18101501464844\n",
      "======================================================================\n",
      "epoch: 7 -- step: 0 -- mse_loss: 51.056678771972656\n",
      "======================================================================\n",
      "epoch: 7 -- step: 5 -- mse_loss: 55.68798065185547\n",
      "======================================================================\n",
      "epoch: 8 -- step: 0 -- mse_loss: 103.42742919921875\n",
      "======================================================================\n",
      "epoch: 8 -- step: 5 -- mse_loss: 53.1341438293457\n",
      "======================================================================\n",
      "epoch: 9 -- step: 0 -- mse_loss: 50.70848846435547\n",
      "======================================================================\n",
      "epoch: 9 -- step: 5 -- mse_loss: 55.99867248535156\n",
      "======================================================================\n",
      "epoch: 10 -- step: 0 -- mse_loss: 63.156822204589844\n",
      "======================================================================\n",
      "epoch: 10 -- step: 5 -- mse_loss: 45.78254318237305\n",
      "======================================================================\n",
      "epoch: 11 -- step: 0 -- mse_loss: 54.06938934326172\n",
      "======================================================================\n",
      "epoch: 11 -- step: 5 -- mse_loss: 64.26001739501953\n",
      "======================================================================\n",
      "epoch: 12 -- step: 0 -- mse_loss: 55.24372482299805\n",
      "======================================================================\n",
      "epoch: 12 -- step: 5 -- mse_loss: 50.651344299316406\n",
      "======================================================================\n",
      "epoch: 13 -- step: 0 -- mse_loss: 94.65579223632812\n",
      "======================================================================\n",
      "epoch: 13 -- step: 5 -- mse_loss: 40.318851470947266\n",
      "======================================================================\n",
      "epoch: 14 -- step: 0 -- mse_loss: 65.73455047607422\n",
      "======================================================================\n",
      "epoch: 14 -- step: 5 -- mse_loss: 108.9559097290039\n",
      "======================================================================\n",
      "epoch: 15 -- step: 0 -- mse_loss: 53.8370361328125\n",
      "======================================================================\n",
      "epoch: 15 -- step: 5 -- mse_loss: 64.24368286132812\n",
      "======================================================================\n",
      "epoch: 16 -- step: 0 -- mse_loss: 45.881649017333984\n",
      "======================================================================\n",
      "epoch: 16 -- step: 5 -- mse_loss: 50.908531188964844\n",
      "======================================================================\n",
      "epoch: 17 -- step: 0 -- mse_loss: 50.465415954589844\n",
      "======================================================================\n",
      "epoch: 17 -- step: 5 -- mse_loss: 44.56037902832031\n",
      "======================================================================\n",
      "epoch: 18 -- step: 0 -- mse_loss: 56.768096923828125\n",
      "======================================================================\n",
      "epoch: 18 -- step: 5 -- mse_loss: 93.35041809082031\n",
      "======================================================================\n",
      "epoch: 19 -- step: 0 -- mse_loss: 52.846866607666016\n",
      "======================================================================\n",
      "epoch: 19 -- step: 5 -- mse_loss: 61.13111877441406\n",
      "======================================================================\n",
      "epoch: 20 -- step: 0 -- mse_loss: 56.38285446166992\n",
      "======================================================================\n",
      "epoch: 20 -- step: 5 -- mse_loss: 46.63955307006836\n",
      "======================================================================\n",
      "epoch: 21 -- step: 0 -- mse_loss: 62.92003631591797\n",
      "======================================================================\n",
      "epoch: 21 -- step: 5 -- mse_loss: 39.28058624267578\n",
      "======================================================================\n",
      "epoch: 22 -- step: 0 -- mse_loss: 80.33439636230469\n",
      "======================================================================\n",
      "epoch: 22 -- step: 5 -- mse_loss: 39.6655387878418\n",
      "======================================================================\n",
      "epoch: 23 -- step: 0 -- mse_loss: 63.59552001953125\n",
      "======================================================================\n",
      "epoch: 23 -- step: 5 -- mse_loss: 56.25315856933594\n",
      "======================================================================\n",
      "epoch: 24 -- step: 0 -- mse_loss: 45.482872009277344\n",
      "======================================================================\n",
      "epoch: 24 -- step: 5 -- mse_loss: 89.6885986328125\n",
      "======================================================================\n",
      "epoch: 25 -- step: 0 -- mse_loss: 95.7711410522461\n",
      "======================================================================\n",
      "epoch: 25 -- step: 5 -- mse_loss: 48.63105773925781\n",
      "======================================================================\n",
      "epoch: 26 -- step: 0 -- mse_loss: 42.78056335449219\n",
      "======================================================================\n",
      "epoch: 26 -- step: 5 -- mse_loss: 61.92057418823242\n",
      "======================================================================\n",
      "epoch: 27 -- step: 0 -- mse_loss: 56.47582244873047\n",
      "======================================================================\n",
      "epoch: 27 -- step: 5 -- mse_loss: 50.65047836303711\n",
      "======================================================================\n",
      "epoch: 28 -- step: 0 -- mse_loss: 55.24100112915039\n",
      "======================================================================\n",
      "epoch: 28 -- step: 5 -- mse_loss: 46.952110290527344\n",
      "======================================================================\n",
      "epoch: 29 -- step: 0 -- mse_loss: 47.649208068847656\n",
      "======================================================================\n",
      "epoch: 29 -- step: 5 -- mse_loss: 56.85662078857422\n",
      "======================================================================\n",
      "epoch: 30 -- step: 0 -- mse_loss: 88.8453369140625\n",
      "======================================================================\n",
      "epoch: 30 -- step: 5 -- mse_loss: 50.77569580078125\n",
      "======================================================================\n",
      "epoch: 31 -- step: 0 -- mse_loss: 61.830543518066406\n",
      "======================================================================\n",
      "epoch: 31 -- step: 5 -- mse_loss: 46.59180450439453\n",
      "======================================================================\n",
      "epoch: 32 -- step: 0 -- mse_loss: 64.89437866210938\n",
      "======================================================================\n",
      "epoch: 32 -- step: 5 -- mse_loss: 73.994384765625\n",
      "======================================================================\n",
      "epoch: 33 -- step: 0 -- mse_loss: 65.60633850097656\n",
      "======================================================================\n",
      "epoch: 33 -- step: 5 -- mse_loss: 71.18443298339844\n",
      "======================================================================\n",
      "epoch: 34 -- step: 0 -- mse_loss: 37.87871551513672\n",
      "======================================================================\n",
      "epoch: 34 -- step: 5 -- mse_loss: 70.35152435302734\n",
      "======================================================================\n",
      "epoch: 35 -- step: 0 -- mse_loss: 51.68074035644531\n",
      "======================================================================\n",
      "epoch: 35 -- step: 5 -- mse_loss: 47.12939453125\n",
      "======================================================================\n",
      "epoch: 36 -- step: 0 -- mse_loss: 44.300838470458984\n",
      "======================================================================\n",
      "epoch: 36 -- step: 5 -- mse_loss: 62.53669738769531\n",
      "======================================================================\n",
      "epoch: 37 -- step: 0 -- mse_loss: 55.85731887817383\n",
      "======================================================================\n",
      "epoch: 37 -- step: 5 -- mse_loss: 41.02277374267578\n",
      "======================================================================\n",
      "epoch: 38 -- step: 0 -- mse_loss: 81.81570434570312\n",
      "======================================================================\n",
      "epoch: 38 -- step: 5 -- mse_loss: 58.14900207519531\n",
      "======================================================================\n",
      "epoch: 39 -- step: 0 -- mse_loss: 55.73877716064453\n",
      "======================================================================\n",
      "epoch: 39 -- step: 5 -- mse_loss: 51.798973083496094\n",
      "======================================================================\n",
      "epoch: 40 -- step: 0 -- mse_loss: 79.10757446289062\n",
      "======================================================================\n",
      "epoch: 40 -- step: 5 -- mse_loss: 71.69758605957031\n",
      "======================================================================\n",
      "epoch: 41 -- step: 0 -- mse_loss: 103.3838882446289\n",
      "======================================================================\n",
      "epoch: 41 -- step: 5 -- mse_loss: 58.828147888183594\n",
      "======================================================================\n",
      "epoch: 42 -- step: 0 -- mse_loss: 77.81379699707031\n",
      "======================================================================\n",
      "epoch: 42 -- step: 5 -- mse_loss: 59.640045166015625\n",
      "======================================================================\n",
      "epoch: 43 -- step: 0 -- mse_loss: 78.458984375\n",
      "======================================================================\n",
      "epoch: 43 -- step: 5 -- mse_loss: 46.73440170288086\n",
      "======================================================================\n",
      "epoch: 44 -- step: 0 -- mse_loss: 82.22940063476562\n",
      "======================================================================\n",
      "epoch: 44 -- step: 5 -- mse_loss: 52.25994110107422\n",
      "======================================================================\n",
      "epoch: 45 -- step: 0 -- mse_loss: 68.69648742675781\n",
      "======================================================================\n",
      "epoch: 45 -- step: 5 -- mse_loss: 49.22627258300781\n",
      "======================================================================\n",
      "epoch: 46 -- step: 0 -- mse_loss: 48.52296447753906\n",
      "======================================================================\n",
      "epoch: 46 -- step: 5 -- mse_loss: 54.921470642089844\n",
      "======================================================================\n",
      "epoch: 47 -- step: 0 -- mse_loss: 57.84265899658203\n",
      "======================================================================\n",
      "epoch: 47 -- step: 5 -- mse_loss: 41.78643035888672\n",
      "======================================================================\n",
      "epoch: 48 -- step: 0 -- mse_loss: 60.673824310302734\n",
      "======================================================================\n",
      "epoch: 48 -- step: 5 -- mse_loss: 64.59927368164062\n",
      "======================================================================\n",
      "epoch: 49 -- step: 0 -- mse_loss: 71.2462158203125\n",
      "======================================================================\n",
      "epoch: 49 -- step: 5 -- mse_loss: 62.860435485839844\n",
      "======================================================================\n",
      "epoch: 50 -- step: 0 -- mse_loss: 91.9334487915039\n",
      "======================================================================\n",
      "epoch: 50 -- step: 5 -- mse_loss: 55.09837341308594\n",
      "======================================================================\n",
      "epoch: 51 -- step: 0 -- mse_loss: 54.54166030883789\n",
      "======================================================================\n",
      "epoch: 51 -- step: 5 -- mse_loss: 52.81847381591797\n",
      "======================================================================\n",
      "epoch: 52 -- step: 0 -- mse_loss: 63.19384002685547\n",
      "======================================================================\n",
      "epoch: 52 -- step: 5 -- mse_loss: 68.20173645019531\n",
      "======================================================================\n",
      "epoch: 53 -- step: 0 -- mse_loss: 55.3685302734375\n",
      "======================================================================\n",
      "epoch: 53 -- step: 5 -- mse_loss: 36.64939498901367\n",
      "======================================================================\n",
      "epoch: 54 -- step: 0 -- mse_loss: 54.114723205566406\n",
      "======================================================================\n",
      "epoch: 54 -- step: 5 -- mse_loss: 72.56546020507812\n",
      "======================================================================\n",
      "epoch: 55 -- step: 0 -- mse_loss: 62.656124114990234\n",
      "======================================================================\n",
      "epoch: 55 -- step: 5 -- mse_loss: 69.55567932128906\n",
      "======================================================================\n",
      "epoch: 56 -- step: 0 -- mse_loss: 54.48351287841797\n",
      "======================================================================\n",
      "epoch: 56 -- step: 5 -- mse_loss: 58.71919250488281\n",
      "======================================================================\n",
      "epoch: 57 -- step: 0 -- mse_loss: 44.34217071533203\n",
      "======================================================================\n",
      "epoch: 57 -- step: 5 -- mse_loss: 58.46916961669922\n",
      "======================================================================\n",
      "epoch: 58 -- step: 0 -- mse_loss: 63.496238708496094\n",
      "======================================================================\n",
      "epoch: 58 -- step: 5 -- mse_loss: 68.12081146240234\n",
      "======================================================================\n",
      "epoch: 59 -- step: 0 -- mse_loss: 63.67201232910156\n",
      "======================================================================\n",
      "epoch: 59 -- step: 5 -- mse_loss: 61.92085266113281\n",
      "======================================================================\n",
      "epoch: 60 -- step: 0 -- mse_loss: 62.580352783203125\n",
      "======================================================================\n",
      "epoch: 60 -- step: 5 -- mse_loss: 75.25343322753906\n",
      "======================================================================\n",
      "epoch: 61 -- step: 0 -- mse_loss: 52.90447235107422\n",
      "======================================================================\n",
      "epoch: 61 -- step: 5 -- mse_loss: 62.188751220703125\n",
      "======================================================================\n",
      "epoch: 62 -- step: 0 -- mse_loss: 59.90983581542969\n",
      "======================================================================\n",
      "epoch: 62 -- step: 5 -- mse_loss: 86.86666870117188\n",
      "======================================================================\n",
      "epoch: 63 -- step: 0 -- mse_loss: 60.81706237792969\n",
      "======================================================================\n",
      "epoch: 63 -- step: 5 -- mse_loss: 46.84930419921875\n",
      "======================================================================\n",
      "epoch: 64 -- step: 0 -- mse_loss: 83.96138000488281\n",
      "======================================================================\n",
      "epoch: 64 -- step: 5 -- mse_loss: 56.69707489013672\n",
      "======================================================================\n",
      "epoch: 65 -- step: 0 -- mse_loss: 74.6729965209961\n",
      "======================================================================\n",
      "epoch: 65 -- step: 5 -- mse_loss: 58.80396270751953\n",
      "======================================================================\n",
      "epoch: 66 -- step: 0 -- mse_loss: 83.35172271728516\n",
      "======================================================================\n",
      "epoch: 66 -- step: 5 -- mse_loss: 49.874855041503906\n",
      "======================================================================\n",
      "epoch: 67 -- step: 0 -- mse_loss: 43.296531677246094\n",
      "======================================================================\n",
      "epoch: 67 -- step: 5 -- mse_loss: 64.61557006835938\n",
      "======================================================================\n",
      "epoch: 68 -- step: 0 -- mse_loss: 63.815547943115234\n",
      "======================================================================\n",
      "epoch: 68 -- step: 5 -- mse_loss: 53.41902542114258\n",
      "======================================================================\n",
      "epoch: 69 -- step: 0 -- mse_loss: 54.238914489746094\n",
      "======================================================================\n",
      "epoch: 69 -- step: 5 -- mse_loss: 52.25438690185547\n",
      "======================================================================\n",
      "epoch: 70 -- step: 0 -- mse_loss: 62.57693099975586\n",
      "======================================================================\n",
      "epoch: 70 -- step: 5 -- mse_loss: 53.99468994140625\n",
      "======================================================================\n",
      "epoch: 71 -- step: 0 -- mse_loss: 62.69845199584961\n",
      "======================================================================\n",
      "epoch: 71 -- step: 5 -- mse_loss: 54.06327438354492\n",
      "======================================================================\n",
      "epoch: 72 -- step: 0 -- mse_loss: 59.87196350097656\n",
      "======================================================================\n",
      "epoch: 72 -- step: 5 -- mse_loss: 68.82908630371094\n",
      "======================================================================\n",
      "epoch: 73 -- step: 0 -- mse_loss: 59.06893539428711\n",
      "======================================================================\n",
      "epoch: 73 -- step: 5 -- mse_loss: 73.76969909667969\n",
      "======================================================================\n",
      "epoch: 74 -- step: 0 -- mse_loss: 62.377437591552734\n",
      "======================================================================\n",
      "epoch: 74 -- step: 5 -- mse_loss: 48.23969268798828\n",
      "======================================================================\n",
      "epoch: 75 -- step: 0 -- mse_loss: 49.87218475341797\n",
      "======================================================================\n",
      "epoch: 75 -- step: 5 -- mse_loss: 74.10684204101562\n",
      "======================================================================\n",
      "epoch: 76 -- step: 0 -- mse_loss: 40.830909729003906\n",
      "======================================================================\n",
      "epoch: 76 -- step: 5 -- mse_loss: 67.30416870117188\n",
      "======================================================================\n",
      "epoch: 77 -- step: 0 -- mse_loss: 40.69803237915039\n",
      "======================================================================\n",
      "epoch: 77 -- step: 5 -- mse_loss: 55.028480529785156\n",
      "======================================================================\n",
      "epoch: 78 -- step: 0 -- mse_loss: 69.51564025878906\n",
      "======================================================================\n",
      "epoch: 78 -- step: 5 -- mse_loss: 63.74474334716797\n",
      "======================================================================\n",
      "epoch: 79 -- step: 0 -- mse_loss: 77.73662567138672\n",
      "======================================================================\n",
      "epoch: 79 -- step: 5 -- mse_loss: 63.23512649536133\n",
      "======================================================================\n",
      "epoch: 80 -- step: 0 -- mse_loss: 55.15141296386719\n",
      "======================================================================\n",
      "epoch: 80 -- step: 5 -- mse_loss: 43.69834899902344\n",
      "======================================================================\n",
      "epoch: 81 -- step: 0 -- mse_loss: 62.75695037841797\n",
      "======================================================================\n",
      "epoch: 81 -- step: 5 -- mse_loss: 50.47254943847656\n",
      "======================================================================\n",
      "epoch: 82 -- step: 0 -- mse_loss: 72.74683380126953\n",
      "======================================================================\n",
      "epoch: 82 -- step: 5 -- mse_loss: 50.32918930053711\n",
      "======================================================================\n",
      "epoch: 83 -- step: 0 -- mse_loss: 69.51473236083984\n",
      "======================================================================\n",
      "epoch: 83 -- step: 5 -- mse_loss: 53.253475189208984\n",
      "======================================================================\n",
      "epoch: 84 -- step: 0 -- mse_loss: 60.26485824584961\n",
      "======================================================================\n",
      "epoch: 84 -- step: 5 -- mse_loss: 54.16878890991211\n",
      "======================================================================\n",
      "epoch: 85 -- step: 0 -- mse_loss: 46.06375503540039\n",
      "======================================================================\n",
      "epoch: 85 -- step: 5 -- mse_loss: 45.834930419921875\n",
      "======================================================================\n",
      "epoch: 86 -- step: 0 -- mse_loss: 45.983253479003906\n",
      "======================================================================\n",
      "epoch: 86 -- step: 5 -- mse_loss: 75.85346221923828\n",
      "======================================================================\n",
      "epoch: 87 -- step: 0 -- mse_loss: 65.32337951660156\n",
      "======================================================================\n",
      "epoch: 87 -- step: 5 -- mse_loss: 51.35667419433594\n",
      "======================================================================\n",
      "epoch: 88 -- step: 0 -- mse_loss: 51.243446350097656\n",
      "======================================================================\n",
      "epoch: 88 -- step: 5 -- mse_loss: 80.0763931274414\n",
      "======================================================================\n",
      "epoch: 89 -- step: 0 -- mse_loss: 54.363006591796875\n",
      "======================================================================\n",
      "epoch: 89 -- step: 5 -- mse_loss: 40.15280532836914\n",
      "======================================================================\n",
      "epoch: 90 -- step: 0 -- mse_loss: 90.4045181274414\n",
      "======================================================================\n",
      "epoch: 90 -- step: 5 -- mse_loss: 61.290096282958984\n",
      "======================================================================\n",
      "epoch: 91 -- step: 0 -- mse_loss: 54.325721740722656\n",
      "======================================================================\n",
      "epoch: 91 -- step: 5 -- mse_loss: 38.11410140991211\n",
      "======================================================================\n",
      "epoch: 92 -- step: 0 -- mse_loss: 59.480560302734375\n",
      "======================================================================\n",
      "epoch: 92 -- step: 5 -- mse_loss: 64.8000259399414\n",
      "======================================================================\n",
      "epoch: 93 -- step: 0 -- mse_loss: 74.63021850585938\n",
      "======================================================================\n",
      "epoch: 93 -- step: 5 -- mse_loss: 53.944725036621094\n",
      "======================================================================\n",
      "epoch: 94 -- step: 0 -- mse_loss: 58.981529235839844\n",
      "======================================================================\n",
      "epoch: 94 -- step: 5 -- mse_loss: 32.41719055175781\n",
      "======================================================================\n",
      "epoch: 95 -- step: 0 -- mse_loss: 41.028785705566406\n",
      "======================================================================\n",
      "epoch: 95 -- step: 5 -- mse_loss: 76.5440902709961\n",
      "======================================================================\n",
      "epoch: 96 -- step: 0 -- mse_loss: 70.09925079345703\n",
      "======================================================================\n",
      "epoch: 96 -- step: 5 -- mse_loss: 59.584571838378906\n",
      "======================================================================\n",
      "epoch: 97 -- step: 0 -- mse_loss: 50.41291427612305\n",
      "======================================================================\n",
      "epoch: 97 -- step: 5 -- mse_loss: 54.988624572753906\n",
      "======================================================================\n",
      "epoch: 98 -- step: 0 -- mse_loss: 54.22983169555664\n",
      "======================================================================\n",
      "epoch: 98 -- step: 5 -- mse_loss: 41.606781005859375\n",
      "======================================================================\n",
      "epoch: 99 -- step: 0 -- mse_loss: 47.42803955078125\n",
      "======================================================================\n",
      "epoch: 99 -- step: 5 -- mse_loss: 53.62528991699219\n",
      "======================================================================\n",
      "epoch: 100 -- step: 0 -- mse_loss: 45.32054901123047\n",
      "======================================================================\n",
      "epoch: 100 -- step: 5 -- mse_loss: 74.69400024414062\n",
      "======================================================================\n",
      "epoch: 101 -- step: 0 -- mse_loss: 64.32884979248047\n",
      "======================================================================\n",
      "epoch: 101 -- step: 5 -- mse_loss: 62.8188362121582\n",
      "======================================================================\n",
      "epoch: 102 -- step: 0 -- mse_loss: 86.72834777832031\n",
      "======================================================================\n",
      "epoch: 102 -- step: 5 -- mse_loss: 43.6123046875\n",
      "======================================================================\n",
      "epoch: 103 -- step: 0 -- mse_loss: 58.55426788330078\n",
      "======================================================================\n",
      "epoch: 103 -- step: 5 -- mse_loss: 48.83525848388672\n",
      "======================================================================\n",
      "epoch: 104 -- step: 0 -- mse_loss: 65.81346893310547\n",
      "======================================================================\n",
      "epoch: 104 -- step: 5 -- mse_loss: 72.63077545166016\n",
      "======================================================================\n",
      "epoch: 105 -- step: 0 -- mse_loss: 55.04594039916992\n",
      "======================================================================\n",
      "epoch: 105 -- step: 5 -- mse_loss: 54.262413024902344\n",
      "======================================================================\n",
      "epoch: 106 -- step: 0 -- mse_loss: 51.80054473876953\n",
      "======================================================================\n",
      "epoch: 106 -- step: 5 -- mse_loss: 34.76529312133789\n",
      "======================================================================\n",
      "epoch: 107 -- step: 0 -- mse_loss: 60.57793426513672\n",
      "======================================================================\n",
      "epoch: 107 -- step: 5 -- mse_loss: 74.95787811279297\n",
      "======================================================================\n",
      "epoch: 108 -- step: 0 -- mse_loss: 56.606475830078125\n",
      "======================================================================\n",
      "epoch: 108 -- step: 5 -- mse_loss: 59.69011688232422\n",
      "======================================================================\n",
      "epoch: 109 -- step: 0 -- mse_loss: 75.59323120117188\n",
      "======================================================================\n",
      "epoch: 109 -- step: 5 -- mse_loss: 52.89921951293945\n",
      "======================================================================\n",
      "epoch: 110 -- step: 0 -- mse_loss: 53.759803771972656\n",
      "======================================================================\n",
      "epoch: 110 -- step: 5 -- mse_loss: 74.13139343261719\n",
      "======================================================================\n",
      "epoch: 111 -- step: 0 -- mse_loss: 55.368064880371094\n",
      "======================================================================\n",
      "epoch: 111 -- step: 5 -- mse_loss: 56.93556594848633\n",
      "======================================================================\n",
      "epoch: 112 -- step: 0 -- mse_loss: 46.42366027832031\n",
      "======================================================================\n",
      "epoch: 112 -- step: 5 -- mse_loss: 61.26302719116211\n",
      "======================================================================\n",
      "epoch: 113 -- step: 0 -- mse_loss: 91.69054412841797\n",
      "======================================================================\n",
      "epoch: 113 -- step: 5 -- mse_loss: 51.37481689453125\n",
      "======================================================================\n",
      "epoch: 114 -- step: 0 -- mse_loss: 69.09495544433594\n",
      "======================================================================\n",
      "epoch: 114 -- step: 5 -- mse_loss: 43.57708740234375\n",
      "======================================================================\n",
      "epoch: 115 -- step: 0 -- mse_loss: 59.63554000854492\n",
      "======================================================================\n",
      "epoch: 115 -- step: 5 -- mse_loss: 50.11910629272461\n",
      "======================================================================\n",
      "epoch: 116 -- step: 0 -- mse_loss: 62.715797424316406\n",
      "======================================================================\n",
      "epoch: 116 -- step: 5 -- mse_loss: 48.83662033081055\n",
      "======================================================================\n",
      "epoch: 117 -- step: 0 -- mse_loss: 59.6805305480957\n",
      "======================================================================\n",
      "epoch: 117 -- step: 5 -- mse_loss: 43.29467010498047\n",
      "======================================================================\n",
      "epoch: 118 -- step: 0 -- mse_loss: 66.41891479492188\n",
      "======================================================================\n",
      "epoch: 118 -- step: 5 -- mse_loss: 40.51441192626953\n",
      "======================================================================\n",
      "epoch: 119 -- step: 0 -- mse_loss: 55.82548522949219\n",
      "======================================================================\n",
      "epoch: 119 -- step: 5 -- mse_loss: 73.65283203125\n",
      "======================================================================\n",
      "epoch: 120 -- step: 0 -- mse_loss: 64.85531616210938\n",
      "======================================================================\n",
      "epoch: 120 -- step: 5 -- mse_loss: 49.06438446044922\n",
      "======================================================================\n",
      "epoch: 121 -- step: 0 -- mse_loss: 77.64480590820312\n",
      "======================================================================\n",
      "epoch: 121 -- step: 5 -- mse_loss: 58.579891204833984\n",
      "======================================================================\n",
      "epoch: 122 -- step: 0 -- mse_loss: 58.87343978881836\n",
      "======================================================================\n",
      "epoch: 122 -- step: 5 -- mse_loss: 56.495994567871094\n",
      "======================================================================\n",
      "epoch: 123 -- step: 0 -- mse_loss: 61.538124084472656\n",
      "======================================================================\n",
      "epoch: 123 -- step: 5 -- mse_loss: 69.99964904785156\n",
      "======================================================================\n",
      "epoch: 124 -- step: 0 -- mse_loss: 41.36893844604492\n",
      "======================================================================\n",
      "epoch: 124 -- step: 5 -- mse_loss: 43.62012481689453\n",
      "======================================================================\n",
      "epoch: 125 -- step: 0 -- mse_loss: 83.5825424194336\n",
      "======================================================================\n",
      "epoch: 125 -- step: 5 -- mse_loss: 62.4716796875\n",
      "======================================================================\n",
      "epoch: 126 -- step: 0 -- mse_loss: 57.57408905029297\n",
      "======================================================================\n",
      "epoch: 126 -- step: 5 -- mse_loss: 52.41628646850586\n",
      "======================================================================\n",
      "epoch: 127 -- step: 0 -- mse_loss: 65.50285339355469\n",
      "======================================================================\n",
      "epoch: 127 -- step: 5 -- mse_loss: 59.5844612121582\n",
      "======================================================================\n",
      "epoch: 128 -- step: 0 -- mse_loss: 51.132293701171875\n",
      "======================================================================\n",
      "epoch: 128 -- step: 5 -- mse_loss: 50.05290603637695\n",
      "======================================================================\n",
      "epoch: 129 -- step: 0 -- mse_loss: 76.95015716552734\n",
      "======================================================================\n",
      "epoch: 129 -- step: 5 -- mse_loss: 58.33596420288086\n",
      "======================================================================\n",
      "epoch: 130 -- step: 0 -- mse_loss: 44.895957946777344\n",
      "======================================================================\n",
      "epoch: 130 -- step: 5 -- mse_loss: 55.12773895263672\n",
      "======================================================================\n",
      "epoch: 131 -- step: 0 -- mse_loss: 71.82040405273438\n",
      "======================================================================\n",
      "epoch: 131 -- step: 5 -- mse_loss: 38.171966552734375\n",
      "======================================================================\n",
      "epoch: 132 -- step: 0 -- mse_loss: 46.40060043334961\n",
      "======================================================================\n",
      "epoch: 132 -- step: 5 -- mse_loss: 77.0430908203125\n",
      "======================================================================\n",
      "epoch: 133 -- step: 0 -- mse_loss: 51.564388275146484\n",
      "======================================================================\n",
      "epoch: 133 -- step: 5 -- mse_loss: 45.28590393066406\n",
      "======================================================================\n",
      "epoch: 134 -- step: 0 -- mse_loss: 71.72259521484375\n",
      "======================================================================\n",
      "epoch: 134 -- step: 5 -- mse_loss: 46.68963623046875\n",
      "======================================================================\n",
      "epoch: 135 -- step: 0 -- mse_loss: 54.17420959472656\n",
      "======================================================================\n",
      "epoch: 135 -- step: 5 -- mse_loss: 45.89879608154297\n",
      "======================================================================\n",
      "epoch: 136 -- step: 0 -- mse_loss: 62.66011047363281\n",
      "======================================================================\n",
      "epoch: 136 -- step: 5 -- mse_loss: 59.78811264038086\n",
      "======================================================================\n",
      "epoch: 137 -- step: 0 -- mse_loss: 52.97547149658203\n",
      "======================================================================\n",
      "epoch: 137 -- step: 5 -- mse_loss: 52.854248046875\n",
      "======================================================================\n",
      "epoch: 138 -- step: 0 -- mse_loss: 65.72454071044922\n",
      "======================================================================\n",
      "epoch: 138 -- step: 5 -- mse_loss: 52.68006134033203\n",
      "======================================================================\n",
      "epoch: 139 -- step: 0 -- mse_loss: 52.7073860168457\n",
      "======================================================================\n",
      "epoch: 139 -- step: 5 -- mse_loss: 52.402259826660156\n",
      "======================================================================\n",
      "epoch: 140 -- step: 0 -- mse_loss: 55.154727935791016\n",
      "======================================================================\n",
      "epoch: 140 -- step: 5 -- mse_loss: 68.8721923828125\n",
      "======================================================================\n",
      "epoch: 141 -- step: 0 -- mse_loss: 51.9267578125\n",
      "======================================================================\n",
      "epoch: 141 -- step: 5 -- mse_loss: 59.417720794677734\n",
      "======================================================================\n",
      "epoch: 142 -- step: 0 -- mse_loss: 62.81084060668945\n",
      "======================================================================\n",
      "epoch: 142 -- step: 5 -- mse_loss: 48.80843734741211\n",
      "======================================================================\n",
      "epoch: 143 -- step: 0 -- mse_loss: 55.800323486328125\n",
      "======================================================================\n",
      "epoch: 143 -- step: 5 -- mse_loss: 48.14311981201172\n",
      "======================================================================\n",
      "epoch: 144 -- step: 0 -- mse_loss: 81.19217681884766\n",
      "======================================================================\n",
      "epoch: 144 -- step: 5 -- mse_loss: 42.72908020019531\n",
      "======================================================================\n",
      "epoch: 145 -- step: 0 -- mse_loss: 44.68656921386719\n",
      "======================================================================\n",
      "epoch: 145 -- step: 5 -- mse_loss: 52.49608612060547\n",
      "======================================================================\n",
      "epoch: 146 -- step: 0 -- mse_loss: 66.7723388671875\n",
      "======================================================================\n",
      "epoch: 146 -- step: 5 -- mse_loss: 68.75022888183594\n",
      "======================================================================\n",
      "epoch: 147 -- step: 0 -- mse_loss: 44.74549102783203\n",
      "======================================================================\n",
      "epoch: 147 -- step: 5 -- mse_loss: 53.91937255859375\n",
      "======================================================================\n",
      "epoch: 148 -- step: 0 -- mse_loss: 56.66142272949219\n",
      "======================================================================\n",
      "epoch: 148 -- step: 5 -- mse_loss: 79.56305694580078\n",
      "======================================================================\n",
      "epoch: 149 -- step: 0 -- mse_loss: 59.86842727661133\n",
      "======================================================================\n",
      "epoch: 149 -- step: 5 -- mse_loss: 45.253089904785156\n",
      "======================================================================\n",
      "epoch: 150 -- step: 0 -- mse_loss: 51.27349090576172\n",
      "======================================================================\n",
      "epoch: 150 -- step: 5 -- mse_loss: 76.0595474243164\n",
      "======================================================================\n",
      "epoch: 151 -- step: 0 -- mse_loss: 56.303321838378906\n",
      "======================================================================\n",
      "epoch: 151 -- step: 5 -- mse_loss: 49.25022888183594\n",
      "======================================================================\n",
      "epoch: 152 -- step: 0 -- mse_loss: 65.64531707763672\n",
      "======================================================================\n",
      "epoch: 152 -- step: 5 -- mse_loss: 66.18824768066406\n",
      "======================================================================\n",
      "epoch: 153 -- step: 0 -- mse_loss: 35.83277893066406\n",
      "======================================================================\n",
      "epoch: 153 -- step: 5 -- mse_loss: 48.68651580810547\n",
      "======================================================================\n",
      "epoch: 154 -- step: 0 -- mse_loss: 73.734619140625\n",
      "======================================================================\n",
      "epoch: 154 -- step: 5 -- mse_loss: 59.2181282043457\n",
      "======================================================================\n",
      "epoch: 155 -- step: 0 -- mse_loss: 73.40605163574219\n",
      "======================================================================\n",
      "epoch: 155 -- step: 5 -- mse_loss: 69.53905487060547\n",
      "======================================================================\n",
      "epoch: 156 -- step: 0 -- mse_loss: 70.59944152832031\n",
      "======================================================================\n",
      "epoch: 156 -- step: 5 -- mse_loss: 75.1467056274414\n",
      "======================================================================\n",
      "epoch: 157 -- step: 0 -- mse_loss: 53.82700729370117\n",
      "======================================================================\n",
      "epoch: 157 -- step: 5 -- mse_loss: 49.364036560058594\n",
      "======================================================================\n",
      "epoch: 158 -- step: 0 -- mse_loss: 53.051063537597656\n",
      "======================================================================\n",
      "epoch: 158 -- step: 5 -- mse_loss: 41.71373748779297\n",
      "======================================================================\n",
      "epoch: 159 -- step: 0 -- mse_loss: 64.97567749023438\n",
      "======================================================================\n",
      "epoch: 159 -- step: 5 -- mse_loss: 52.00560760498047\n",
      "======================================================================\n",
      "epoch: 160 -- step: 0 -- mse_loss: 65.45204162597656\n",
      "======================================================================\n",
      "epoch: 160 -- step: 5 -- mse_loss: 61.70621871948242\n",
      "======================================================================\n",
      "epoch: 161 -- step: 0 -- mse_loss: 66.2869644165039\n",
      "======================================================================\n",
      "epoch: 161 -- step: 5 -- mse_loss: 44.572227478027344\n",
      "======================================================================\n",
      "epoch: 162 -- step: 0 -- mse_loss: 70.49073028564453\n",
      "======================================================================\n",
      "epoch: 162 -- step: 5 -- mse_loss: 36.864601135253906\n",
      "======================================================================\n",
      "epoch: 163 -- step: 0 -- mse_loss: 48.34663772583008\n",
      "======================================================================\n",
      "epoch: 163 -- step: 5 -- mse_loss: 53.55256652832031\n",
      "======================================================================\n",
      "epoch: 164 -- step: 0 -- mse_loss: 54.857513427734375\n",
      "======================================================================\n",
      "epoch: 164 -- step: 5 -- mse_loss: 55.067691802978516\n",
      "======================================================================\n",
      "epoch: 165 -- step: 0 -- mse_loss: 50.73375701904297\n",
      "======================================================================\n",
      "epoch: 165 -- step: 5 -- mse_loss: 55.67218780517578\n",
      "======================================================================\n",
      "epoch: 166 -- step: 0 -- mse_loss: 52.88185119628906\n",
      "======================================================================\n",
      "epoch: 166 -- step: 5 -- mse_loss: 74.03019714355469\n",
      "======================================================================\n",
      "epoch: 167 -- step: 0 -- mse_loss: 56.13523864746094\n",
      "======================================================================\n",
      "epoch: 167 -- step: 5 -- mse_loss: 54.28663635253906\n",
      "======================================================================\n",
      "epoch: 168 -- step: 0 -- mse_loss: 78.51873016357422\n",
      "======================================================================\n",
      "epoch: 168 -- step: 5 -- mse_loss: 48.36737823486328\n",
      "======================================================================\n",
      "epoch: 169 -- step: 0 -- mse_loss: 53.562992095947266\n",
      "======================================================================\n",
      "epoch: 169 -- step: 5 -- mse_loss: 47.84368896484375\n",
      "======================================================================\n",
      "epoch: 170 -- step: 0 -- mse_loss: 56.17108154296875\n",
      "======================================================================\n",
      "epoch: 170 -- step: 5 -- mse_loss: 81.84318542480469\n",
      "======================================================================\n",
      "epoch: 171 -- step: 0 -- mse_loss: 86.21528625488281\n",
      "======================================================================\n",
      "epoch: 171 -- step: 5 -- mse_loss: 91.72895812988281\n",
      "======================================================================\n",
      "epoch: 172 -- step: 0 -- mse_loss: 56.02520751953125\n",
      "======================================================================\n",
      "epoch: 172 -- step: 5 -- mse_loss: 64.14126586914062\n",
      "======================================================================\n",
      "epoch: 173 -- step: 0 -- mse_loss: 76.67474365234375\n",
      "======================================================================\n",
      "epoch: 173 -- step: 5 -- mse_loss: 76.33033752441406\n",
      "======================================================================\n",
      "epoch: 174 -- step: 0 -- mse_loss: 68.14945983886719\n",
      "======================================================================\n",
      "epoch: 174 -- step: 5 -- mse_loss: 51.1893310546875\n",
      "======================================================================\n",
      "epoch: 175 -- step: 0 -- mse_loss: 80.7366943359375\n",
      "======================================================================\n",
      "epoch: 175 -- step: 5 -- mse_loss: 80.77684783935547\n",
      "======================================================================\n",
      "epoch: 176 -- step: 0 -- mse_loss: 53.89753723144531\n",
      "======================================================================\n",
      "epoch: 176 -- step: 5 -- mse_loss: 54.94007110595703\n",
      "======================================================================\n",
      "epoch: 177 -- step: 0 -- mse_loss: 73.48202514648438\n",
      "======================================================================\n",
      "epoch: 177 -- step: 5 -- mse_loss: 58.05199432373047\n",
      "======================================================================\n",
      "epoch: 178 -- step: 0 -- mse_loss: 74.12980651855469\n",
      "======================================================================\n",
      "epoch: 178 -- step: 5 -- mse_loss: 55.93523406982422\n",
      "======================================================================\n",
      "epoch: 179 -- step: 0 -- mse_loss: 70.15617370605469\n",
      "======================================================================\n",
      "epoch: 179 -- step: 5 -- mse_loss: 45.881961822509766\n",
      "======================================================================\n",
      "epoch: 180 -- step: 0 -- mse_loss: 61.05857849121094\n",
      "======================================================================\n",
      "epoch: 180 -- step: 5 -- mse_loss: 63.616188049316406\n",
      "======================================================================\n",
      "epoch: 181 -- step: 0 -- mse_loss: 60.51087188720703\n",
      "======================================================================\n",
      "epoch: 181 -- step: 5 -- mse_loss: 56.127845764160156\n",
      "======================================================================\n",
      "epoch: 182 -- step: 0 -- mse_loss: 68.60562896728516\n",
      "======================================================================\n",
      "epoch: 182 -- step: 5 -- mse_loss: 56.78716278076172\n",
      "======================================================================\n",
      "epoch: 183 -- step: 0 -- mse_loss: 64.55250549316406\n",
      "======================================================================\n",
      "epoch: 183 -- step: 5 -- mse_loss: 45.28047180175781\n",
      "======================================================================\n",
      "epoch: 184 -- step: 0 -- mse_loss: 52.76182556152344\n",
      "======================================================================\n",
      "epoch: 184 -- step: 5 -- mse_loss: 48.921913146972656\n",
      "======================================================================\n",
      "epoch: 185 -- step: 0 -- mse_loss: 61.47378158569336\n",
      "======================================================================\n",
      "epoch: 185 -- step: 5 -- mse_loss: 52.09742736816406\n",
      "======================================================================\n",
      "epoch: 186 -- step: 0 -- mse_loss: 38.46576690673828\n",
      "======================================================================\n",
      "epoch: 186 -- step: 5 -- mse_loss: 59.35560607910156\n",
      "======================================================================\n",
      "epoch: 187 -- step: 0 -- mse_loss: 69.71171569824219\n",
      "======================================================================\n",
      "epoch: 187 -- step: 5 -- mse_loss: 49.911170959472656\n",
      "======================================================================\n",
      "epoch: 188 -- step: 0 -- mse_loss: 73.45539855957031\n",
      "======================================================================\n",
      "epoch: 188 -- step: 5 -- mse_loss: 82.01937103271484\n",
      "======================================================================\n",
      "epoch: 189 -- step: 0 -- mse_loss: 52.41560363769531\n",
      "======================================================================\n",
      "epoch: 189 -- step: 5 -- mse_loss: 64.16741943359375\n",
      "======================================================================\n",
      "epoch: 190 -- step: 0 -- mse_loss: 61.85966491699219\n",
      "======================================================================\n",
      "epoch: 190 -- step: 5 -- mse_loss: 53.32353210449219\n",
      "======================================================================\n",
      "epoch: 191 -- step: 0 -- mse_loss: 48.149497985839844\n",
      "======================================================================\n",
      "epoch: 191 -- step: 5 -- mse_loss: 36.00144577026367\n",
      "======================================================================\n",
      "epoch: 192 -- step: 0 -- mse_loss: 45.279510498046875\n",
      "======================================================================\n",
      "epoch: 192 -- step: 5 -- mse_loss: 54.035484313964844\n",
      "======================================================================\n",
      "epoch: 193 -- step: 0 -- mse_loss: 60.88616180419922\n",
      "======================================================================\n",
      "epoch: 193 -- step: 5 -- mse_loss: 44.313602447509766\n",
      "======================================================================\n",
      "epoch: 194 -- step: 0 -- mse_loss: 43.80695343017578\n",
      "======================================================================\n",
      "epoch: 194 -- step: 5 -- mse_loss: 58.74031448364258\n",
      "======================================================================\n",
      "epoch: 195 -- step: 0 -- mse_loss: 41.812198638916016\n",
      "======================================================================\n",
      "epoch: 195 -- step: 5 -- mse_loss: 78.66899871826172\n",
      "======================================================================\n",
      "epoch: 196 -- step: 0 -- mse_loss: 59.173709869384766\n",
      "======================================================================\n",
      "epoch: 196 -- step: 5 -- mse_loss: 60.87487030029297\n",
      "======================================================================\n",
      "epoch: 197 -- step: 0 -- mse_loss: 63.358421325683594\n",
      "======================================================================\n",
      "epoch: 197 -- step: 5 -- mse_loss: 53.678653717041016\n",
      "======================================================================\n",
      "epoch: 198 -- step: 0 -- mse_loss: 64.44206237792969\n",
      "======================================================================\n",
      "epoch: 198 -- step: 5 -- mse_loss: 49.46232604980469\n",
      "======================================================================\n",
      "epoch: 199 -- step: 0 -- mse_loss: 52.383216857910156\n",
      "======================================================================\n",
      "epoch: 199 -- step: 5 -- mse_loss: 66.63002014160156\n",
      "======================================================================\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "optimzier = keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "for epoch in range(200):\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            out = model(x)\n",
    "            mse_loss = tf.reduce_mean(keras.losses.MSE(y, out))\n",
    "            mae_loss = tf.reduce_mean(keras.losses.MAE(y, out))\n",
    "        if step % 5 == 0:\n",
    "            print(f\"epoch: {epoch} -- step: {step} -- mse_loss: {mse_loss}\")\n",
    "            print(\"=\" * 70)\n",
    "        grads = tape.gradient(mse_loss, model.trainable_variables)\n",
    "        optimzier.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Variable 'dense_12/kernel:0' shape=(9, 64) dtype=float32, numpy=\n array([[-1.52300194e-01, -1.74181253e-01, -3.34539711e-01,\n         -3.07454765e-01,  3.30851585e-01, -1.20471660e-02,\n         -1.35033235e-01,  1.31627452e-02, -2.58101206e-02,\n         -5.49390307e-03, -3.39912057e-01, -1.31741181e-01,\n          2.70920724e-01, -1.42729133e-01, -1.11079045e-01,\n          2.93168128e-01,  8.03056061e-02, -6.18683957e-02,\n         -2.43860886e-01,  1.22649699e-01, -3.15426499e-01,\n         -2.27682009e-01, -2.86196589e-01,  3.25116843e-01,\n         -2.25095868e-01,  2.25060806e-02, -8.94799903e-02,\n          1.79325297e-01, -1.87382042e-01,  1.34255618e-01,\n          2.12203726e-01,  1.36059925e-01, -1.49603246e-03,\n         -3.28183442e-01,  2.79951274e-01,  2.45501827e-02,\n          1.32497296e-01, -3.30474645e-01, -1.66068360e-01,\n          2.97869712e-01,  2.84900427e-01,  2.51890838e-01,\n          1.88502267e-01,  2.27429047e-01, -1.78945914e-01,\n          2.88927943e-01,  6.28240481e-02, -3.16402316e-03,\n          1.09943211e-01, -8.56033191e-02, -3.32458347e-01,\n         -2.48834714e-01, -1.26119271e-01, -2.60410696e-01,\n          2.32407674e-01,  2.16348335e-01,  2.11112171e-01,\n         -3.94906178e-02, -7.01992437e-02, -3.55845541e-02,\n         -2.22930253e-01,  1.77970603e-01, -2.69455343e-01,\n         -2.39695936e-01],\n        [-1.14521347e-01,  1.55034855e-01, -2.50753284e-01,\n          1.87086873e-02, -2.85591483e-01, -2.01459318e-01,\n          1.91300929e-01,  7.86121190e-02, -3.61252278e-02,\n          2.11337566e-01, -2.79738605e-01,  2.45497704e-01,\n          1.47087686e-02, -1.59182951e-01, -2.28296235e-01,\n          3.19161803e-01, -1.61484286e-01,  2.20038816e-01,\n          2.50936538e-01,  3.00865769e-01, -2.57212818e-01,\n         -2.76729435e-01, -2.23863140e-01,  1.70435235e-01,\n         -1.57374308e-01, -2.04487696e-01,  2.26789042e-01,\n          3.21523011e-01, -1.63685650e-01,  2.54454702e-01,\n          1.77201718e-01,  2.42160589e-01, -1.26062054e-02,\n         -1.61834329e-01, -2.02787086e-01,  4.84639406e-02,\n         -3.60694617e-01, -9.04694125e-02, -1.63353443e-01,\n          2.21292265e-02, -2.30867907e-01, -6.93534538e-02,\n          1.57782122e-01, -2.70025462e-01,  1.41811684e-01,\n         -2.01311022e-01,  6.45533428e-02,  2.65689403e-01,\n         -3.09830517e-01, -3.20958793e-01, -3.23305607e-01,\n          2.10463405e-01, -3.16441387e-01,  7.41724744e-02,\n          9.82509181e-02, -2.52438873e-01, -2.37534478e-01,\n          1.53324172e-01,  8.01294222e-02, -6.49063364e-02,\n          1.72006518e-01,  1.87652826e-01,  1.54847100e-01,\n         -1.33798748e-01],\n        [-1.69133186e-01,  1.63908079e-01, -1.97417632e-01,\n         -3.31742227e-01,  1.07086644e-01,  3.48259136e-02,\n         -1.82716578e-01,  4.37312648e-02,  2.21249282e-01,\n          4.94473316e-02, -1.69292802e-03, -3.81230265e-02,\n         -1.88703537e-01,  1.37635052e-01,  1.74575612e-01,\n          4.77933958e-02,  2.76492059e-01,  1.33018315e-01,\n         -2.98371762e-01, -1.68794647e-01, -1.60377324e-01,\n          1.57157406e-01, -1.10034205e-01,  2.40642220e-01,\n         -3.47925603e-01,  1.68010294e-02,  2.04287708e-01,\n          2.78090358e-01,  1.89607777e-02,  1.33468993e-02,\n         -2.74686724e-01, -2.43263870e-01,  2.69475251e-01,\n          2.59732734e-02,  1.50491700e-01,  1.14837568e-02,\n         -2.95059025e-01,  2.95344852e-02,  1.76767752e-01,\n          4.47360799e-02, -1.52645275e-01, -8.08395445e-02,\n          1.66748047e-01,  9.05435160e-02, -2.04058424e-01,\n         -2.45387539e-01, -4.85610180e-02,  2.50524759e-01,\n         -2.78688461e-01, -4.24502864e-02,  2.52128094e-01,\n         -2.53132373e-01,  2.45420575e-01, -2.46908903e-01,\n         -3.68140377e-02, -1.95189655e-01, -2.75830537e-01,\n         -1.17017910e-01,  1.56101972e-01, -3.02612215e-01,\n          5.81408245e-03,  2.79888302e-01, -3.02889436e-01,\n          5.97116053e-02],\n        [-2.38839135e-01,  2.16454014e-01, -2.58310914e-01,\n         -2.17713669e-01,  1.02353329e-02,  9.73022263e-03,\n         -3.06470990e-01,  1.26046529e-02, -1.31646499e-01,\n         -1.44988954e-01, -1.16431616e-01,  1.38206586e-01,\n          2.32912190e-02, -1.47132158e-01, -7.20286369e-02,\n          2.77710050e-01, -3.26175302e-01,  2.57221013e-01,\n          2.56496221e-01,  2.22336963e-01,  2.00025439e-01,\n         -4.11468074e-02,  1.93909451e-01, -6.24644570e-02,\n          9.40286890e-02, -2.41835833e-01,  1.49304181e-01,\n          2.27154672e-01,  1.57032802e-04,  5.40769612e-03,\n          3.12430233e-01, -1.21718291e-02,  6.43141940e-02,\n          3.56235318e-02,  2.39782676e-01, -1.90603156e-02,\n         -2.49205798e-01, -1.90614477e-01,  6.53828308e-02,\n         -1.18698925e-01,  1.85406655e-01, -3.62318121e-02,\n         -1.15791671e-01,  7.67908469e-02, -3.23418140e-01,\n          1.40742406e-01,  3.21689159e-01, -2.31867686e-01,\n         -1.14530876e-01, -1.39502853e-01,  1.32312909e-01,\n         -1.08057618e-01, -2.04331785e-01,  2.64206767e-01,\n          3.05392176e-01,  1.88907832e-01, -2.68713325e-01,\n          1.64475203e-01, -7.80078322e-02, -1.77354485e-01,\n         -2.15910487e-02,  1.03379562e-01, -1.41732544e-02,\n         -2.86794484e-01],\n        [ 9.24223959e-02,  3.59738261e-01, -1.79827765e-01,\n          3.04053009e-01, -2.38655359e-01,  1.07927196e-01,\n         -2.07114190e-01,  9.88449976e-02, -7.94582963e-02,\n         -4.14240137e-02,  9.01886821e-02,  3.66351634e-01,\n          1.28679061e-02,  4.18891609e-02,  1.29048359e-02,\n         -1.51227772e-01, -2.48244151e-01, -1.25194117e-01,\n          1.83788419e-01, -2.30397522e-01, -3.89309004e-02,\n         -3.47295105e-01,  1.16235957e-01, -1.74545482e-01,\n          1.49680823e-01, -1.34314135e-01, -9.31199715e-02,\n         -1.09015286e-01,  2.20275909e-01,  2.59512872e-01,\n         -1.96661893e-02,  1.32701784e-01,  1.04683816e-01,\n         -7.23159239e-02,  9.02124215e-03, -2.70699888e-01,\n          3.09691221e-01, -5.75699881e-02, -8.12735260e-02,\n          1.46581337e-01,  4.82321195e-02, -1.69650525e-01,\n         -1.38320222e-01, -2.61569679e-01, -2.67886162e-01,\n          2.89542973e-01,  1.73136875e-01, -2.56207108e-01,\n          4.65787277e-02,  4.24169526e-02, -2.03312621e-01,\n         -2.28864610e-01, -1.72974274e-01, -2.92473704e-01,\n          1.49708107e-01,  3.09555024e-01, -4.48611267e-02,\n         -1.51551366e-01, -3.32740158e-01, -8.79438668e-02,\n          6.69647232e-02, -8.18412155e-02, -9.85112563e-02,\n         -1.75328448e-01],\n        [-3.58556099e-02,  2.77668297e-01, -3.38902086e-01,\n          3.68640125e-02,  3.49997371e-01,  3.49558473e-01,\n         -2.70290256e-01,  1.73311815e-01,  3.15154254e-01,\n         -5.93695231e-02,  1.41627014e-01,  9.57976803e-02,\n         -5.27262166e-02, -3.47714484e-01,  9.67227221e-02,\n         -2.44121477e-01,  2.47701213e-01, -2.54708469e-01,\n          1.46559596e-01,  2.91677803e-01,  6.91581666e-02,\n          3.09366826e-03, -6.49954751e-02,  4.27054539e-02,\n          1.67009741e-01,  1.70656875e-01, -2.54340738e-01,\n          7.63601959e-02,  3.29046138e-02,  3.07829320e-01,\n         -2.37710893e-01,  1.95436358e-01, -1.02049343e-01,\n          5.24860062e-02, -4.76700515e-02,  1.54296905e-01,\n          1.56820744e-01, -3.24424386e-01,  4.65994477e-02,\n          2.70091426e-02, -2.57073671e-01,  7.22419322e-02,\n         -2.69158602e-01, -3.39118958e-01,  2.62193769e-01,\n         -8.11050311e-02, -2.08850298e-02,  1.74951218e-02,\n         -2.92103112e-01,  8.78738612e-02,  2.34831348e-01,\n         -2.03081608e-01, -2.90283978e-01, -4.70275320e-02,\n         -8.96120742e-02, -7.66131207e-02,  2.87290752e-01,\n          1.41592294e-01,  1.77414939e-02,  2.08949849e-01,\n         -1.44906715e-01,  2.25506097e-01, -3.67845535e-01,\n          3.21909308e-01],\n        [-1.89525157e-01,  3.78479958e-01, -2.72097830e-02,\n          4.53565642e-03, -3.14743370e-01,  2.11680308e-01,\n          1.84973449e-01,  2.28554964e-01, -7.30051100e-02,\n          3.52121770e-01,  2.87960678e-01,  1.36743948e-01,\n          1.74846664e-01, -1.06560893e-01, -2.23975368e-02,\n          4.00420010e-01, -1.77779570e-01,  1.15458488e-01,\n          2.20595047e-01,  3.37480217e-01, -3.44002485e-01,\n         -3.33229780e-01,  2.71251887e-01,  7.82002062e-02,\n          1.96483299e-01, -1.17435999e-01,  1.04893617e-01,\n         -5.82572855e-02, -2.12390631e-01,  5.07032797e-02,\n          1.63320690e-01,  1.99052617e-01,  1.06711298e-01,\n         -3.29367340e-01,  2.97772288e-01,  2.03772292e-01,\n         -2.60819077e-01, -1.48347944e-01,  3.36097151e-01,\n          2.72117347e-01,  6.98642284e-02, -2.88434118e-01,\n         -1.84732497e-01, -2.87809312e-01,  1.58285186e-01,\n          4.20553416e-01, -1.50466844e-01, -1.70734763e-01,\n         -1.87600821e-01, -1.79959431e-01, -3.02615047e-01,\n          2.89466996e-02, -1.21513270e-01,  1.94376841e-01,\n          2.89219409e-01,  3.39678913e-01,  4.06299829e-02,\n         -2.45779425e-01, -2.64233593e-02,  8.80261213e-02,\n         -1.28083289e-01,  3.07755530e-01,  1.73880726e-01,\n          1.29566586e-03],\n        [-3.44011843e-01, -7.32268542e-02,  2.87869930e-01,\n         -2.49879826e-02, -1.39919579e-01, -1.80446595e-01,\n         -2.43016668e-02, -2.91483343e-01,  1.66326553e-01,\n         -3.40316556e-02,  3.29980224e-01,  3.14166658e-02,\n         -3.58117640e-01,  5.21457903e-02,  2.89644063e-01,\n         -2.74287403e-01, -1.66637334e-03, -2.92095542e-01,\n         -4.28119421e-01, -1.67933211e-01, -7.89864920e-03,\n         -1.43738911e-01,  2.30023339e-01, -4.01457250e-01,\n         -8.81707892e-02,  2.36369729e-01, -3.48651022e-01,\n         -4.30762291e-01,  2.64823228e-01,  8.77980292e-02,\n          1.32675529e-01,  1.75132588e-01, -3.06062996e-01,\n          1.89540401e-01, -2.66591400e-01, -3.50221545e-01,\n          1.78068846e-01, -9.66008008e-02, -2.79483259e-01,\n         -4.38020110e-01, -1.13703914e-01, -2.57502586e-01,\n         -3.27320069e-01,  3.27496260e-01, -3.56218010e-01,\n         -2.91896224e-01, -1.48504555e-01, -1.57785356e-01,\n         -1.48155659e-01,  1.63210645e-01,  6.87170476e-02,\n         -3.37259144e-01,  2.49269411e-01,  1.15072273e-01,\n          1.66717231e-01,  2.34960765e-01, -2.41067968e-02,\n          2.05413163e-01,  4.64139655e-02,  2.91839242e-01,\n          2.63018727e-01, -3.87648381e-02, -4.19069469e-01,\n         -2.77583241e-01],\n        [ 2.78278440e-01, -3.90345991e-01, -1.64109051e-01,\n          2.66080257e-03,  2.85331219e-01,  2.83506840e-01,\n          2.64188051e-01, -4.26727355e-01, -1.42999604e-01,\n         -2.38046139e-01, -1.15776263e-01, -3.42962205e-01,\n         -1.20657466e-01, -2.47098252e-01,  2.87293077e-01,\n          1.50494175e-02,  1.59127995e-01,  2.56359756e-01,\n         -2.69818008e-01, -3.16414446e-01, -1.74584624e-03,\n          1.51228249e-01, -2.61318833e-01,  2.03542247e-01,\n         -3.21290344e-01, -2.16874346e-01, -7.64529109e-02,\n         -4.64316048e-02,  2.97777176e-01, -1.72967732e-01,\n         -3.33445132e-01, -1.93111598e-01, -1.62633225e-01,\n          4.96284012e-03, -1.54926544e-02, -2.55895883e-01,\n          8.73715505e-02,  1.63175017e-01,  4.37754579e-02,\n         -3.62527072e-01, -8.71434957e-02,  2.50281572e-01,\n         -1.48866206e-01,  2.65313298e-01,  6.78655058e-02,\n         -3.41202885e-01, -4.03187990e-01,  2.83515573e-01,\n         -2.03933090e-01,  1.80335268e-01,  2.73275990e-02,\n          1.44463405e-02,  1.40315786e-01, -3.08125824e-01,\n         -2.94738621e-01, -2.83722937e-01, -2.13915333e-01,\n         -5.66757433e-02, -2.08842069e-01,  2.61662245e-01,\n          2.47109756e-01,  1.62129849e-01,  3.04162085e-01,\n         -3.17646384e-01]], dtype=float32)>,\n <tf.Variable 'dense_12/bias:0' shape=(64,) dtype=float32, numpy=\n array([0.12377671, 0.15001313, 0.14045423, 0.13113861, 0.09241886,\n        0.12717764, 0.14402178, 0.15159458, 0.08594449, 0.08582471,\n        0.11680424, 0.14903447, 0.09805824, 0.11094139, 0.11497076,\n        0.15910587, 0.144059  , 0.1573575 , 0.15255009, 0.15265489,\n        0.12144426, 0.13935143, 0.16489446, 0.1436986 , 0.15943962,\n        0.08558134, 0.1591975 , 0.16085497, 0.12067379, 0.04250393,\n        0.15454985, 0.03105259, 0.1555553 , 0.12984909, 0.15871796,\n        0.15385878, 0.13403006, 0.15082787, 0.06601705, 0.15183581,\n        0.10268914, 0.06509876, 0.09008097, 0.10894343, 0.16576771,\n        0.15692356, 0.1540488 , 0.08349672, 0.15727298, 0.07748725,\n        0.12701136, 0.10092507, 0.10116047, 0.15913521, 0.15633029,\n        0.16575855, 0.15737203, 0.00214419, 0.15703052, 0.13846274,\n        0.12427633, 0.15712774, 0.15827382, 0.12648913], dtype=float32)>,\n <tf.Variable 'dense_13/kernel:0' shape=(64, 64) dtype=float32, numpy=\n array([[-0.02480921,  0.2185629 ,  0.16196366, ...,  0.17456044,\n          0.24338354, -0.05645898],\n        [ 0.23243721, -0.02631675,  0.12052771, ..., -0.08567003,\n          0.33154112,  0.23335849],\n        [-0.05636191,  0.32673195,  0.22847731, ...,  0.1765314 ,\n          0.00718939, -0.0868791 ],\n        ...,\n        [-0.12527688,  0.0023973 ,  0.20263313, ..., -0.07371365,\n         -0.10066609, -0.01716838],\n        [ 0.19706668,  0.26498982,  0.00363429, ..., -0.16548312,\n          0.13897398,  0.32068932],\n        [ 0.14607795,  0.29093418, -0.01679234, ..., -0.08487213,\n          0.05684585,  0.06465811]], dtype=float32)>,\n <tf.Variable 'dense_13/bias:0' shape=(64,) dtype=float32, numpy=\n array([ 0.14969239,  0.14868672,  0.148421  , -0.02840226, -0.0625664 ,\n        -0.04123642, -0.03579615, -0.03561088, -0.04686462, -0.04850537,\n         0.14888145,  0.14932212,  0.1478077 ,  0.0964338 ,  0.15056238,\n         0.14813931, -0.05031047, -0.02992331,  0.1501516 ,  0.1486993 ,\n         0.14895913, -0.06080136,  0.15002987,  0.14805086,  0.1492239 ,\n        -0.0065944 , -0.0443535 ,  0.1477892 ,  0.1480384 , -0.01330134,\n        -0.03924675,  0.13842033,  0.14805812,  0.14879076, -0.03433769,\n        -0.00098798,  0.14888111,  0.        ,  0.1484287 , -0.05925842,\n        -0.01565178,  0.14771962,  0.10997183,  0.12699737,  0.14846948,\n         0.14839147,  0.147643  , -0.00679946,  0.07236109,  0.14885105,\n        -0.06489522, -0.04597382,  0.14802743,  0.14813271,  0.14900093,\n         0.15023582,  0.14906208, -0.05258014, -0.02497121,  0.14175639,\n         0.1482321 , -0.02060818,  0.14792384,  0.1484545 ], dtype=float32)>,\n <tf.Variable 'dense_14/kernel:0' shape=(64, 1) dtype=float32, numpy=\n array([[ 0.17353363],\n        [ 0.28596166],\n        [ 0.31627193],\n        [-0.00763434],\n        [-0.02387391],\n        [-0.1641568 ],\n        [-0.15378943],\n        [-0.23740034],\n        [-0.19477673],\n        [-0.09946984],\n        [ 0.13744152],\n        [ 0.19902222],\n        [ 0.3684299 ],\n        [ 0.10176045],\n        [ 0.13776448],\n        [ 0.42852166],\n        [-0.01425725],\n        [-0.20047739],\n        [ 0.1263405 ],\n        [ 0.1749174 ],\n        [ 0.1819608 ],\n        [-0.12396341],\n        [ 0.13927206],\n        [ 0.38260537],\n        [ 0.14117214],\n        [-0.25885934],\n        [-0.24339792],\n        [ 0.35422218],\n        [ 0.33617613],\n        [-0.07808909],\n        [-0.12954919],\n        [ 0.1130076 ],\n        [ 0.31022632],\n        [ 0.15321517],\n        [-0.21577051],\n        [-0.14165328],\n        [ 0.18560442],\n        [ 0.12757322],\n        [ 0.24755058],\n        [-0.03032911],\n        [-0.21727663],\n        [ 0.37508848],\n        [ 0.09836449],\n        [ 0.10151927],\n        [ 0.25239912],\n        [ 0.16866353],\n        [ 0.32272595],\n        [-0.3199515 ],\n        [ 0.09753674],\n        [ 0.1833813 ],\n        [-0.1553856 ],\n        [-0.03677526],\n        [ 0.36082363],\n        [ 0.41976115],\n        [ 0.17691109],\n        [ 0.12903339],\n        [ 0.18108499],\n        [-0.00592645],\n        [-0.19787948],\n        [ 0.41935742],\n        [ 0.29680866],\n        [-0.21773186],\n        [ 0.2821788 ],\n        [ 0.21943185]], dtype=float32)>,\n <tf.Variable 'dense_14/bias:0' shape=(1,) dtype=float32, numpy=array([0.1474104], dtype=float32)>]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 68
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}